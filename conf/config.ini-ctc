#configuration file
#[acoustic_network_params]
--init-scale = 0.01
--learning-rate = 0.5
--lr_decay_factor = 0.5
--grad_clip = 5
--time_major = True

#[training]
# Whether to read config settings if pre-existing ones are found in checkpoint path
--use_config_file_if_checkpoint_exists = True
# Frequency at which to save the model
--steps_per_checkpoint = 100
# Frequency at which to evaluate on the test set. This must be a multiple of steps_per_checkpoint
#steps_per_evaluation = 1000
--print-trainable-variables = False
--use-normal = False
--use-sgd = True
--restore-training = True
--checkpoint_dir = data-ctc-cv/checkpoints/
--num_threads = 1
--queue_cache = 100
--grad_clip = 5.0
--batch_size = 32

--state_is_tuple = True
--nnet_conf = conf/nnet.conf

#--num_frames_batch = 20

# max_input_seq_length is the maximum number of 0.01s chunks we allow in a single training (or test) example
# max_target_seq_length is the maximum number of letters we allow in a single training (or test) example output
#   Must be less than 65535 (int16 for reduced memory consumption)
# Advices =
#  - Those two values need to be consistent one with the other
#  - These values depends on your dataset used for training
#  - For reduced memory consumption it is possible to reduce batch_size

--max_input_seq_length = 1500
--max_target_seq_length = 600
--skip_frame = 3

# Train data and source.
--tr-scp = train-data/abc.scp
--tr-label = train-data/merge_sort_cv.labels

--cv-label= train-data/merge_sort_cv.labels
--cv-scp = train-data/abc.scp

--feature-transfile = feat_process/transdir/new_final.feature_transform
# Feature 
#--num_streams = 100
--restore_training = True

# Apply batch normalization to the data during training (True / False)
# batch_normalization = False

#[logging]
# Set a log file, if void then log messages will be outputed to the screen
--log_file = data-ctc-cv/full_data.log
#--checkpoint_dir = old-data-ctc-model/checkpoints/
# Set a log level = DEBUG, INFO, WARNING (default) , ERROR or CRITICAL
--log_level = INFO
